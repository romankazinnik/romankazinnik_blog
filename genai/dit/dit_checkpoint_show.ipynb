{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd1a581e-f6aa-4bb1-a7a8-15c93f161184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import copy\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers.models import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf35be-7f57-444b-b5ab-800e1242b959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a9bec-a332-41a5-a18d-cc0e1b296fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b04d40-d317-416a-a559-9f52cfac67e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94643237-8029-4dc1-a38d-e088188a332d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "714edfac-e039-4d31-9cfc-eaf841fce04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size =  32\n",
    "lr = 2e-5\n",
    "\n",
    "train_epoch = 1200\n",
    "\n",
    "# data_loader\n",
    "latent_size = 32\n",
    "\n",
    "timesteps = 500\n",
    "patch_size = 2\n",
    "\n",
    "\n",
    "data_set_root = \"../../datasets\"\n",
    "\n",
    "use_cuda = 0 # torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8901dfef-7f63-4459-aef9-cf5ec68bb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, latent_dir):\n",
    "        self.latent_dir = latent_dir\n",
    "        self.latent_files = sorted(os.listdir(latent_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latent_file = self.latent_files[idx]\n",
    "        latent = np.load(os.path.join(self.latent_dir, latent_file))\n",
    "        return torch.tensor(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "373282b3-920d-4e0a-b630-12414bc37072",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGEBAGS = True\n",
    "\n",
    "if EDGEBAGS:\n",
    "    nm = \"edges2shoes\" # edges2handbags\n",
    "    dataset_dir = f\"/home/roman/PycharmProjects/jobs/dandy/pytorch-CycleGAN-and-pix2pix/datasets/{nm}/\" # /class_A\" #\".\"\n",
    "    latent_save_dir = f\"/home/roman/PycharmProjects/jobs/dandy/pytorch-CycleGAN-and-pix2pix/datasets/{nm}_latent\" # \".\"\n",
    "    latent_save_dir_right = latent_save_dir + \"_right\"\n",
    "    latent_save_dir_left = latent_save_dir + \"_left\"\n",
    "\n",
    "    \n",
    "if False:\n",
    "    \n",
    "    data_set_root = \"/media/luke/Quick_Storage/Data/CelebAHQ/image_latents\"\n",
    "    \n",
    "    trainset = LatentDataset(data_set_root)\n",
    "else:    \n",
    "    dir_latent = latent_save_dir_right\n",
    "    \n",
    "    trainset = LatentDataset(dir_latent)# data_set_root)\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f4bb6b-24f3-4cfc-97c4-4acaad966069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_tensor, patch_size=8):\n",
    "    # Get the dimensions of the image tensor\n",
    "    bs, c, h, w = image_tensor.size()\n",
    "    \n",
    "    # Define the Unfold layer with appropriate parameters\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    # Apply Unfold to the image tensor\n",
    "    unfolded = unfold(image_tensor)\n",
    "    \n",
    "    # Reshape the unfolded tensor to match the desired output shape\n",
    "    # Output shape: BSxLxH, where L is the number of patches in each dimension\n",
    "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
    "    \n",
    "    return unfolded\n",
    "\n",
    "\n",
    "def reconstruct_image(patch_sequence, image_shape, patch_size=8):\n",
    "    \"\"\"\n",
    "    Reconstructs the original image tensor from a sequence of patches.\n",
    "\n",
    "    Args:\n",
    "        patch_sequence (torch.Tensor): Sequence of patches with shape\n",
    "        BS x L x (C x patch_size x patch_size)\n",
    "        image_shape (tuple): Shape of the original image tensor (bs, c, h, w).\n",
    "        patch_size (int): Size of the patches used in extraction.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reconstructed image tensor.\n",
    "    \"\"\"\n",
    "    bs, c, h, w = image_shape\n",
    "    num_patches_h = h // patch_size\n",
    "    num_patches_w = w // patch_size\n",
    "    \n",
    "    # Reshape the patch sequence to match the unfolded tensor shape\n",
    "    unfolded_shape = (bs, num_patches_h, num_patches_w, patch_size, patch_size, c)\n",
    "    patch_sequence = patch_sequence.view(*unfolded_shape)\n",
    "    \n",
    "    # Transpose dimensions to match the original image tensor shape\n",
    "    patch_sequence = patch_sequence.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "    \n",
    "    # Reshape the sequence of patches back into the original image tensor shape\n",
    "    reconstructed = patch_sequence.view(bs, c, h, w)\n",
    "    \n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4641134f-2603-42c9-bd30-ad979afbdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalNorm2d(nn.Module):\n",
    "    def __init__(self, hidden_size, num_features):\n",
    "        super(ConditionalNorm2d, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_size, elementwise_affine=False)\n",
    "\n",
    "        self.fcw = nn.Linear(num_features, hidden_size)\n",
    "        self.fcb = nn.Linear(num_features, hidden_size)\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        bs, s, l = x.shape\n",
    "        \n",
    "        out = self.norm(x)\n",
    "        w = self.fcw(features).reshape(bs, 1, -1)\n",
    "        b = self.fcb(features).reshape(bs, 1, -1)\n",
    "\n",
    "        return w * out + b\n",
    "\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "    \n",
    "# Transformer block with self-attention\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, num_features=128):\n",
    "        # Initialize the parent nn.Module\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Layer normalization to normalize the input data\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, \n",
    "                                                    batch_first=True, dropout=0.0)\n",
    "        \n",
    "        # Another layer normalization\n",
    "        self.con_norm = ConditionalNorm2d(hidden_size, num_features)\n",
    "        \n",
    "        # Multi-layer perceptron (MLP) with a hidden layer and activation function\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.LayerNorm(hidden_size * 4),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, x, features):\n",
    "        # Apply the first layer normalization\n",
    "        norm_x = self.norm(x)\n",
    "        \n",
    "        # Apply multi-head attention and add the input (residual connection)\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x)[0] + x\n",
    "        \n",
    "        # Apply the second layer normalization\n",
    "        norm_x = self.con_norm(x, features)\n",
    "        \n",
    "        # Pass through the MLP and add the input (residual connection)\n",
    "        x = self.mlp(norm_x) + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "# Define a Vision Encoder module for the Diffusion Transformer\n",
    "class DiT(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, patch_size=16, \n",
    "                 hidden_size=128, num_features=128, \n",
    "                 num_layers=3, num_heads=4):\n",
    "        super(DiT, self).__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(num_features),\n",
    "            nn.Linear(num_features, 2 * num_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2 * num_features, num_features),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
    "        \n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, channels_in * patch_size * patch_size)\n",
    "                \n",
    "    def forward(self, image_in, index):  \n",
    "        # Get timestep embedding\n",
    "        index_features = self.time_mlp(index)\n",
    "\n",
    "        # Split input into patches\n",
    "        patch_seq = extract_patches(image_in, patch_size=self.patch_size)\n",
    "        patch_emb = self.fc_in(patch_seq)\n",
    "\n",
    "        # Add a unique embedding to each token embedding\n",
    "        embs = patch_emb + self.pos_embedding\n",
    "        \n",
    "        # Pass the embeddings through each Transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, index_features)\n",
    "        \n",
    "        # Project to output\n",
    "        image_out = self.fc_out(embs)\n",
    "        \n",
    "        # Reconstruct the input from patches and return result\n",
    "        return reconstruct_image(image_out, image_in.shape, patch_size=self.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ed3b08a-6fb0-4df6-adb0-5d8b203f14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_alphas_bar(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, steps, steps)\n",
    "    alphas_bar = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_bar = alphas_bar / alphas_bar[0]\n",
    "    return alphas_bar[:timesteps]\n",
    "    \n",
    "def noise_from_x0(curr_img, img_pred, alpha):\n",
    "    return (curr_img - alpha.sqrt() * img_pred)/((1 - alpha).sqrt() + 1e-4)\n",
    "    \n",
    "def cold_diffuse(diffusion_model, sample_in, total_steps, start_step=0):\n",
    "    diffusion_model.eval()\n",
    "    bs = sample_in.shape[0]\n",
    "    alphas = torch.flip(cosine_alphas_bar(total_steps), (0,)).to(device)\n",
    "    random_sample = copy.deepcopy(sample_in)\n",
    "    with torch.no_grad():\n",
    "        for i in trange(start_step, total_steps - 1):\n",
    "            index = (i * torch.ones(bs, device=sample_in.device)).long()\n",
    "\n",
    "            img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "            noise = noise_from_x0(random_sample, img_output, alphas[i])\n",
    "            x0 = img_output\n",
    "\n",
    "            rep1 = alphas[i].sqrt() * x0 + (1 - alphas[i]).sqrt() * noise\n",
    "            rep2 = alphas[i + 1].sqrt() * x0 + (1 - alphas[i + 1]).sqrt() * noise\n",
    "\n",
    "            random_sample += rep2 - rep1\n",
    "\n",
    "        index = ((total_steps - 1) * torch.ones(bs, device=sample_in.device)).long()\n",
    "        img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "    return img_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfcacac8-578f-445d-a979-7bf385a732b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents=torch.Size([32, 4, 32, 32]) latent_size=32\n"
     ]
    }
   ],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = iter(train_loader)\n",
    "# Sample from the itterable object\n",
    "latents = next(dataiter)\n",
    "\n",
    "print(f\"latents={latents.shape} latent_size={latent_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ab9a7b9-4c32-420e-a944-2453338b6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "dit = DiT(latent_size, channels_in=latents.shape[1], patch_size=patch_size, \n",
    "            hidden_size=768, num_layers=10, num_heads=8).to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(dit.parameters(), lr=lr)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "alphas = torch.flip(cosine_alphas_bar(timesteps), (0,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e341f2f-a5da-41bc-8be2-3d73c569f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint\n",
    "def show(dit):\n",
    "    latent_noise = 0.95 * torch.randn(8, 4, latent_size, latent_size, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        with torch.amp.autocast(\"cuda:0\"): \n",
    "            # fake_latents = cold_diffuse(u_net, latent_noise, total_steps=timesteps)\n",
    "            fake_latents = cold_diffuse(dit, latent_noise, total_steps=timesteps)\n",
    "    \n",
    "    vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        with torch.amp.autocast(\"cuda:0\"): \n",
    "            fake_sample = vae.decode(fake_latents / 0.18215).sample\n",
    "            \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    out = vutils.make_grid(fake_sample[:8].detach().float().cpu(), nrow=4, normalize=True)\n",
    "    \n",
    "    _ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "    \n",
    "def load_model(fn=\"latent_dit_100.pt\", device=\"cpu\"):\n",
    "    cp = torch.load(fn)\n",
    "    \n",
    "    # network\n",
    "    dit_100 = DiT(latent_size, channels_in=latents.shape[1], patch_size=patch_size, \n",
    "                hidden_size=768, num_layers=10, num_heads=8).to(device)\n",
    "    \n",
    "    \n",
    "    dit_100.load_state_dict(cp[\"model_state_dict\"])\n",
    "    \n",
    "    optimizer_100 = optim.Adam(dit_100.parameters(), lr=lr)\n",
    "    \n",
    "    optimizer_100.load_state_dict(cp[\"optimizer_state_dict\"])\n",
    "    \n",
    "    loss_log_100 = cp[\"train_data_logger\"]\n",
    "    \n",
    "    start_epoch = cp[\"epoch\"]\n",
    "    \n",
    "    print(f\"start_epoch={start_epoch}\")\n",
    "\n",
    "    return dit_100, optimizer_100, loss_log_100, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee155b14-5359-4fd0-aa1a-5dec05c5c810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2381bea9-ba03-4304-8839-43b233f66da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dit, optimizer, loss_log, start_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatent_dit_2000.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 27\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(fn, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_dit_100.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# network\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     dit_100 \u001b[38;5;241m=\u001b[39m DiT(latent_size, channels_in\u001b[38;5;241m=\u001b[39mlatents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], patch_size\u001b[38;5;241m=\u001b[39mpatch_size, \n\u001b[1;32m     31\u001b[0m                 hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/serialization.py:1516\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/_weights_only_unpickler.py:532\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ):\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    530\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m         )\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    534\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/serialization.py:2078\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2077\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 2078\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/serialization.py:2044\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mdetect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2044\u001b[0m     wrap_storage \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2046\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m location\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/serialization.py:637\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/storage.py:287\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    286\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/jobs/dandy/pytorch_tutorials/uvtutor39cu126/lib/python3.9/site-packages/torch/_utils.py:101\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse, (\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "dit, optimizer, loss_log, start_epoch = load_model(\"latent_dit_2000.pt\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc0a97-4fde-429e-bfdc-3299885150f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(dit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
